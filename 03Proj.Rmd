---
title: "Proj_exp_02"
output: pdf_document
date: "2022-12-05"
---



```{r}
library(tidyverse)
library(tidymodels)
#library("dplyr")
library(stringr)
#library(conflicted)
#conflict_prefer("filter", "dplyr")
tidymodels_prefer()

```


```{r}
data <- read_csv("~/R/KDD-R/Data/final_data(v.2).csv", show_col_types = FALSE)

```

```{r}

medi <- data[, -c( 3, 15, 16)]%>% 
  dplyr::rename(Pop_total = B16010_001E)

```

```{}


# Create a new computed variable
medi$adjusted_pop <- (medi$Pop_total - medi$Margin_error_poptotal)
view(medi)
```

```{r}
#conflict_prefer("filter", "dplyr")
#factor(c(medi$Hosp_oa_rat, medi$Prv_RUCA, medi$DRG_Cd, medi$Emg_Svcs))
#medi$Hosp_oa_rat <- factor(medi$Hosp_oa_rat)
#medi$Prv_RUCA <- factor(medi$Prv_RUCA)
#medi$DRG_Cd <- factor(medi$DRG_Cd)
medi$Emg_Svcs <- factor(medi$Emg_Svcs)
#medi$Prv_CCN <- factor(medi$Prv_CCN)
#medi$Prv_Zip <- factor(medi$Prv_Zip)
medi$State <- factor(medi$State)
medi$Hosp_Own <- factor(medi$Hosp_Own)
medi$Cnt_Name <- factor(medi$Cnt_Name)
medi$Hosp_Name<- factor(medi$Hosp_Name)

view(medi)
```

```{r}
# manulally selected the variables
data <- medi# %>% 
 # select(State, Prv_Zip,Prv_CCN, Cnt_Name, Hosp_Own,Hosp_Name, Prv_RUCA, Hosp_oa_rat, Emg_Svcs, DRG_Cd, Cost_to_pat, adjusted_pop,Pop_total, Margin_error_poptotal)

#view(data)

```

```{r}
#removing any duplicates
data<- data[!duplicated(data$Hosp_Own, data$Hosp_Name),]

```



```{r}
# to split it into AMI dataset
medi_data_AMI <- data %>% 
  filter(DRG_Cd %in% c("246", "247", "248", "249", "250", "251","280","281","282"))
  #filter(DRG_Cd %in% c(282))
glimpse(medi_data_AMI)



```

```{r}
# to create Pnuemonia dataset
medi_data_Pn <- data %>% 
  filter(DRG_Cd %in% c('177', '178', '179', '193', '194', '195'))


glimpse(medi_data_Pn)
```


```{r}
medi_data_AMI %>% 
  ggplot (aes(x = Cost_to_pat)) +
  geom_histogram(bins = 50, col= "white") +
  scale_x_log10()
```


```{r}
# splitting the data into training and testing sets (AMI)
set.seed(1526)
AMI_split <- initial_split(medi_data_AMI, prop = 0.70)
AMI_train <- training(AMI_split)
AMI_test  <- testing(AMI_split)

#cross-validation folds
set.seed(1700)
AMI_folds <- vfold_cv(AMI_train, strata = Cost_to_pat)
AMI_folds

```

```{r}
library(PerformanceAnalytics)
medi_data_AMI %>%
dplyr::select_if(is.numeric) %>%
  chart.Correlation()
```
```{r}
library(GGally)
medi_data_AMI %>%
dplyr::select_if(is.numeric) %>%
    ggpairs()
```

```{r}

# creating the recipe for model

library(embed)

AMI_rec <-recipe(Cost_to_pat ~., data = AMI_train) %>% 
  update_role(Prv_Zip, Hosp_Own,City,new_role = 'id') %>% 
  step_other(State, threshold = 0.01) %>% 
  step_lencode_glm(State,Hosp_Name, outcome = vars(Cost_to_pat)) %>% 
  step_dummy(all_nominal_predictors())

AMI_rec

```
```{}
prep(AMI_rec) %>% 
  tidy(number = 1) %>% 
  filter(level == "..new")

```
```{r}
# xgboost for feature selection
library(xgboost)
#defined model for xgboost
xgb_spec <- boost_tree(trees = tune(),
                       min_n = tune(),
                       mode ="regression",
                       engine = "xgboost",
                       mtry=tune(),
                       learn_rate = 0.01
                       )
```

```{r}
# workflow
library(workflows)

xgb_wf<-workflow(AMI_rec, xgb_spec, bake(new_data = NULL))

```

```{r}
# tuning the hyperparameters for the model

library(finetune)
doParallel::registerDoParallel()

set.seed(420)
xgb_rs<-
  tune_race_anova(
    xgb_wf,
    resamples = AMI_folds,
    grid = 15,
    metric_set("rmse"),
    control = control_race(verbose_elim = TRUE),
    )

xgb_rs

```

```{r}
plot_race(xgb_rs)
```

```{r}
show_best(xgb_rs)
```

```{r}
xgb_last <- xgb_wf %>%
  finalize_workflow(select_best(xgb_rs, "rmse")) %>%
  last_fit(AMI_split)

xgb_last
```
```{r}
collect_predictions(xgb_last)
```
```{r}
library(vip)
extract_workflow(xgb_last) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point", num_features = 5)
```
```{r}
library(vip)
extract_workflow(xgb_last) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point")
```



```{}

tree_spec <- 
  decision_tree() %>%
  set_engine('rpart') %>%  # rpart, c5.0, spark
  set_mode('classification')

```

```{}
tree_wflow <-
 workflow() %>%
 add_recipe(AMI_rec) %>% 
 add_model(tree_spec)
```


```{}
tree_res <- 
  tree_wflow %>% 
  fit_resamples(
    resamples = AMI_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy),
    control = control_resamples(save_pred = TRUE)
    )
```
Ë† 
```{}
library(keras)

nnet_spec <- 
  mlp(epochs = 10, hidden_units = 5, dropout = 0.1) %>% 
  set_mode("classification") %>% 
  set_engine("nnet", verbose = 0)

```

```{}
nnet_wflow <- workflow() %>% 
  add_recipe(AMI_rec) %>% 
  add_model(nnet_spec)
```

```{}
nnet_res <- 
  nnet_wflow %>% 
  fit_resamples(
    resamples = AMI_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy),
    control = control_resamples(save_pred = TRUE)
    )
```

```{}
# standardize the data

#ptk$cdur_scaled <- scale(ptk$cdur)

AMI_train$Cost_to_pat_scaled<- scale(AMI_train$Cost_to_pat)
```



  
```
  




